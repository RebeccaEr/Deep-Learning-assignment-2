{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg as linalg\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers import *\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.calibration as calibration\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import calendar\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the path to the data folder\n",
    "os.chdir('C://Users//Rebecca//Desktop//deep_learning//assignment2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need for embedding cause we already have the vectors\n",
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())#create the corresponding vector from the numbers\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        self.word_list = np.array([])\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                #Please note that word_list was added as it was impossible to access the word  vector as a list\n",
    "                #when running the w2v variable without getting an error (Compatibility Issue)\n",
    "                self.word_list=np.append(self.word_list,word)\n",
    "\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "    \n",
    "        #first stores distances  each being compared word and the distance in arrays\n",
    "        distances=np.array([])\n",
    "        words=np.array([])\n",
    "        \n",
    "        for i in self.word2vec:\n",
    "            returned_score = self.score(w,i)\n",
    "            distances = np.append(distances,returned_score)\n",
    "            words=np.append(words,i)\n",
    "\n",
    "        #Finds the lowest distance with argpartition\n",
    "        #then converts that into a word, using the array\n",
    "        \n",
    "        closest_K_words = words[distances.argsort()[-K:][::-1]] #closest words are the one with highest score: we order them and take the last 5 starting from the one with highest score\n",
    "        return closest_K_words\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        v1=self.word2vec[w1]\n",
    "        v2=self.word2vec[w2]       \n",
    "        return np.dot(v1,v2.T)/(linalg.norm(v1)*linalg.norm(v2))\n",
    "\n",
    "    def translate(self, w, K=5):\n",
    "       # K most similar words: self.score  -  np.argsort\n",
    "    \n",
    "        #this will be used for the translation part: we start from the english word, consider the embedding, transform it with W,\n",
    "        # and compare this result with the french words and find the one that is most similar\n",
    "        distances=np.array([])\n",
    "        words=np.array([])\n",
    "        \n",
    "        for i in self.word2vec:\n",
    "            v2=self.word2vec[i]\n",
    "            returned_score = np.dot(w,v2.T)/(linalg.norm(w)*linalg.norm(v2))\n",
    "            distances = np.append(distances,returned_score)\n",
    "            words=np.append(words,i)\n",
    "\n",
    "        #Finds the lowest distance with argpartition\n",
    "        #then converts that into a word, using the array\n",
    "        \n",
    "        closest_K_words = words[distances.argsort()[-K:][::-1]]\n",
    "        return closest_K_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052403\n",
      "paris france 0.7775108541288561\n",
      "germany berlin 0.7420295235998392\n",
      "['cat' 'cats' 'kitty' 'kitten' 'feline']\n",
      "['dog' 'dogs' 'puppy' 'Dog' 'doggie']\n",
      "['dogs' 'dog' 'Dogs' 'doggies' 'canines']\n",
      "['paris' 'france' 'Paris' 'london' 'berlin']\n",
      "['germany' 'europe' 'german' 'berlin' 'france']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec('crawl-300d-200k.vec', nmax=60000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentence, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "                \n",
    "        sentemb=np.zeros([len(list(w2v.word_list))])            \n",
    "        for word in sentence.split(' '):\n",
    "            #in case the word from the sentence is not part of the word_list (computed earlier) we use the Try Except\n",
    "            try:\n",
    "                    #the below gets the sentemb by looping incrementally increasing the sentemb\n",
    "                #zero vector whenever a word is found in the initial vocab under word_list\n",
    "                sentemb[np.argwhere(w2v.word_list==word).item()]=sentemb[np.argwhere(w2v.word_list==word).item()] + 1\n",
    "            except:\n",
    "                None\n",
    "            #move on to the next sentence\n",
    "\n",
    "        i=0\n",
    "        for sent in sentemb:\n",
    "                #in case we are dowing the mean of word, we simply divide the word count by the \n",
    "                #total number of words in the sentence\n",
    "                if idf==False:\n",
    "                # mean of word vectors\n",
    "                    sentemb[i] = sent / sentemb.sum()\n",
    "                    i=i+1                        \n",
    "                else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                #assert False, 'TODO: fill in the blank' We removed the asset here as we did not \n",
    "                #see a use for it\n",
    "                    sentemb[i] = sent * self.idf_dictionary.get(w2v.word_list[i],0)                \n",
    "                    i=i+1\n",
    "        \n",
    "        return np.vstack(sentemb)\n",
    "        \n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "\n",
    "        ############### This was changed so that the encoding happens in the scoring part of the code, in order\n",
    "        #for the testing examples below don't get encoded twice (given that the score also is called directly)\n",
    "        #by a sentence\n",
    "\n",
    "        keys = sentences\n",
    "        query = s\n",
    "        scores=np.array([])\n",
    "        \n",
    "        for key in keys: \n",
    "            #loops on each individual sentence in sentences, encodes it (under score)\n",
    "            #and appends the score to the scores array \n",
    "            scores = np.append(scores,self.score(key,query,idf))\n",
    "        \n",
    "        most_similar_sentences=np.array([])\n",
    "        \n",
    "        for k in scores.argsort()[-K:][::-1]:\n",
    "            #add  the top k most similar sentences to most_similar sentences\n",
    "            most_similar_sentences = np.append(most_similar_sentences,sentences[k])\n",
    "        \n",
    "        print(most_similar_sentences)\n",
    "        \n",
    "        return most_similar_sentences\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        if s1 and s2:\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        #encodes the sentences and then does the cosine similarity\n",
    "            v1=self.encode(s1,idf)\n",
    "            v2=self.encode(s2,idf)\n",
    "            output = np.dot(v1.T,v2)/(linalg.norm(v1)*linalg.norm(v2))\n",
    "            return output\n",
    "        else:\n",
    "            return 0\n",
    "        #return output\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        self.idf_dictionary = {}\n",
    "        self.idf_flag = {}\n",
    "        for sent in sentences:\n",
    "            for word in sent.split(' '):\n",
    "                if self.idf_flag.get(word,1)==True:\n",
    "                    self.idf_dictionary[word] = self.idf_dictionary.get(word, 0) + 1\n",
    "                    self.idf_flag[word] =False\n",
    "            self.idf_flag={}\n",
    "            \n",
    "        for word in self.idf_dictionary.keys():\n",
    "            self.idf_dictionary[word]= np.log10(len(sentences) / (self.idf_dictionary[word])) \n",
    "        return self.idf_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "5 most similar BoV-mean sentences to : \"  smiling african american boy .  \"\n",
      "[' smiling african american boy . '\n",
      " 'a  year old boy swings in the backyard . '\n",
      " 'a caucasian woman reading a boy scout handbook . '\n",
      " ' girls and one boy playing in the street . '\n",
      " 'a jewish boy behind the star of david . ']\n",
      "score BoV-mean between : \"  man singing and  man playing a saxophone in a concert .  \" and \"  people venture out to go crosscountry skiing .  \"\n",
      "[[0.03661035]]\n",
      "5 most similar idf sentences to : \"  smiling african american boy .  \"\n",
      "[' smiling african american boy . '\n",
      " 'a caucasian woman reading a boy scout handbook . '\n",
      " ' girls and one boy playing in the street . '\n",
      " 'a boy walking wearing a uniform . '\n",
      " 'a jewish boy behind the star of david . ']\n",
      "score BoV-idf between : \"  man singing and  man playing a saxophone in a concert .  \" and \"  people venture out to go crosscountry skiing .  \"\n",
      "[[9.6980462e-05]]\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec('crawl-300d-200k.vec', nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "with open('sentences.txt') as myfile:\n",
    "           sentences=myfile.read().split('\\n')\n",
    "\n",
    "#takes the first 5,000 sentences from the sentence file\n",
    "sentences = sentences[:1000]\n",
    "\n",
    "#removes punctuation and symbols from sentences\n",
    "for x in range(int(len(sentences))):\n",
    "    sentences[x] = re.sub('[0-9]','', sentences[x]).lower()\n",
    "    sentences[x] = re.sub('\\W+\\s+\\\"','', sentences[x])\n",
    "    sentences[x] = re.sub('#','', sentences[x])\n",
    "\n",
    "\n",
    "# Build idf scores for each word\n",
    "#we have changed the below to False, otherwise the IDF is never called and hence\n",
    "#the idf dicitionnary is never built\n",
    "idf = {} if False else s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print('5 most similar BoV-mean sentences to : \"', sentences[10], '\"')\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "\n",
    "print('score BoV-mean between : \"', '' if not sentences else sentences[7], '\" and \"', '' if not sentences else sentences[13], '\"')\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13]))\n",
    "\n",
    "\n",
    "idf = {}  \n",
    "print('5 most similar idf sentences to : \"', sentences[10], '\"')\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "print('score BoV-idf between : \"', '' if not sentences else sentences[7], '\" and \"', '' if not sentences else sentences[13], '\"')\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "en = Word2vec('wiki.en.vec', nmax=50000)\n",
    "fr = Word2vec('wiki.fr.vec', nmax=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "common_words=np.array(list(set(en.word_list).intersection(set(fr.word_list))))\n",
    "\n",
    "words_eng=list(en.word2vec.keys())\n",
    "values_eng=list(en.word2vec.values())\n",
    "words_values_eng=list([words_eng,values_eng])\n",
    "\n",
    "french_words=list(fr.word2vec.keys())\n",
    "french_values=list(fr.word2vec.values())\n",
    "words_values_fr=list([french_words,french_values])\n",
    "\n",
    "english_indices = [words_values_eng[0].index(x) for x in common_words]\n",
    "X=np.array([words_values_eng[1][i] for i in english_indices])\n",
    "\n",
    "french_indices = [words_values_fr[0].index(x) for x in common_words]\n",
    "Y=np.array([words_values_fr[1][i] for i in french_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "def procrustes(X, Y):\n",
    "\n",
    "    # optimum rotation matrix of Y\n",
    "\n",
    "    A = np.dot(X.T, Y)\n",
    "    U,s,Vt = np.linalg.svd(A,full_matrices=False)\n",
    "    V = Vt.T\n",
    "    T = np.dot(V, U.T)\n",
    "\n",
    "    return T\n",
    "\n",
    "W_eng=procrustes(X, Y)\n",
    "W_fr=procrustes(Y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French translation of music ['music' 'musique' 'musiques' 'musical' 'musicale']\n",
      "French translation of love ['love' 'amour' 'lover' 'lovers' 'beautiful']\n",
      "French translation of mother ['mère' 'sœur' 'tante' 'adoptive' 'fille']\n",
      "English translation of liberté ['freedom' 'liberté' 'conscience' 'liberty' 'freedoms']\n",
      "English translation of égalité ['equality' 'inequality' 'inequalities' 'equal' 'affirmative']\n",
      "English translation of fraternité ['fraternity' 'brotherhood' 'fraternal' 'fraternities' 'sorority']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "g=['music','love','mother']\n",
    "for x in g: \n",
    "    gx=en.word2vec[x]\n",
    "    g_new=np.dot(W_eng,gx)\n",
    "    print('French translation of',x,fr.translate(g_new,5))\n",
    "\n",
    "g=['liberté','égalité','fraternité']\n",
    "for x in g:\n",
    "    gx=fr.word2vec[x]\n",
    "    g_new=np.dot(W_fr,gx)\n",
    "    print('English translation of',x,en.translate(g_new,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "os.chdir('C://Users//Rebecca//Desktop//deep_learning//assignment2//nlp_project//data//SST')\n",
    "\n",
    "#loads data from data folder\n",
    "\n",
    "# TYPE CODE HERE\n",
    "with open('stsa.fine.train') as myfile_train:\n",
    "           train_sentences=myfile_train.read().split('\\n')\n",
    "\n",
    "with open('stsa.fine.dev') as myfile_dev:\n",
    "           dev_sentences=myfile_dev.read().split('\\n')\n",
    "        \n",
    "with open('stsa.fine.test.X') as myfile_test:\n",
    "           test_sentences=myfile_test.read().split('\\n')\n",
    "\n",
    "def cleaning_sentences(sentences):\n",
    "    for x in range(int(len(sentences))):\n",
    "        sentences[x] = re.sub('[0-9]','', sentences[x]).lower()\n",
    "        sentences[x] = re.sub('\\W+\\s+\\\"','', sentences[x])\n",
    "        sentences[x] = re.sub('#','', sentences[x])\n",
    "    return sentences\n",
    "\n",
    "def extracting_label_train(train_sentences):\n",
    "    train_labels = list()\n",
    "    train_f = list()\n",
    "    for i in range(len(train_sentences)-1):\n",
    "        train_labels.append(int(train_sentences[i][0]))\n",
    "        train_f.append(train_sentences[i][1:])\n",
    "\n",
    "    return train_labels,train_f\n",
    "\n",
    "train_labels,train_f = extracting_label_train(train_sentences)\n",
    "dev_labels,dev_f = extracting_label_train(dev_sentences)\n",
    "\n",
    "train_f = cleaning_sentences(train_f)\n",
    "dev_f = cleaning_sentences(dev_f)\n",
    "test_sentences = cleaning_sentences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoV TFIdf start\n",
      "Loaded 10000 pretrained word vectors\n",
      "producing sentences embeddings\n",
      "train test\n",
      "encoding dev test\n",
      "encoding test set\n",
      "BoV Average method\n",
      "encoding train test\n",
      "dev test\n",
      "test set\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "os.chdir('C://Users//Rebecca//Desktop//deep_learning//assignment2')\n",
    "\n",
    "\n",
    "print('BoV TFIdf start')\n",
    "w2v = Word2vec('crawl-300d-200k.vec', nmax=10000)\n",
    "s2v_logistic = BoV(w2v)\n",
    "idf = {} if False else s2v_logistic.build_idf(train_f)\n",
    "\n",
    "print('producing sentences embeddings')\n",
    "print('train test')\n",
    "#Produces the sentemb using the idf methodology\n",
    "train_enc_idf = np.zeros([len(train_f),len(w2v.word2id)])\n",
    "for x in range(int(len(train_f))):\n",
    "    train_enc_idf[x] = list(s2v_logistic.encode(train_f[x],s2v_logistic.idf_dictionary))\n",
    "\n",
    "print('encoding dev test')\n",
    "dev_enc_idf = np.zeros([len(dev_f),len(w2v.word2id)])\n",
    "for x in range(int(len(dev_f))):\n",
    "    dev_enc_idf[x] = list(s2v_logistic.encode(dev_f[x],s2v_logistic.idf_dictionary))\n",
    "\n",
    "print('encoding test set')\n",
    "test_enc_idf = np.zeros([len(test_sentences)-1,len(w2v.word2id)])\n",
    "for x in range(int(len(test_sentences))-2):\n",
    "    test_enc_idf[x] = list(s2v_logistic.encode(test_sentences[x],s2v_logistic.idf_dictionary))\n",
    "\n",
    "    \n",
    "print('BoV Average method')\n",
    "    \n",
    "#Produces the sentemb using the average word methodology\n",
    "s2v_logistic_average_word = BoV(w2v)\n",
    "\n",
    "\n",
    "print('encoding train test')\n",
    "train_enc_average_word = np.zeros([len(train_f),len(w2v.word2id)])\n",
    "for x in range(int(len(train_f))):\n",
    "    train_enc_average_word[x] = list(s2v_logistic_average_word.encode(train_f[x]))\n",
    "print('dev test')\n",
    "\n",
    "dev_enc_average_word = np.zeros([len(dev_f),len(w2v.word2id)])\n",
    "for x in range(int(len(dev_f))):\n",
    "    dev_enc_average_word[x] = list(s2v_logistic_average_word.encode(dev_f[x]))\n",
    "print('test set')\n",
    "\n",
    "test_enc_average_word = np.zeros([len(test_sentences)-1,len(w2v.word2id)])\n",
    "for x in range(int(len(test_sentences))-2):\n",
    "    test_enc_average_word[x] = list(s2v_logistic_average_word.encode(test_sentences[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF-Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rebecca\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model\n",
      "Score for Average Model in the training set is  0.5602762172284644  and Score for idf model in training set is 0.8073501872659176\n",
      "Score for Average Model in the dev set is  0.38782924613987285  and Score for idf model in the dev set is 0.3723887375113533\n",
      "Error for Average Model in the dev set is  1.419806844191778  and Error for idf model in the dev set is 1.556918550001064\n",
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='sag',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='sag',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "#different penalties regularization paramaters\n",
    "C =[1,0.5,0.1,0.2,1,2,0.001]\n",
    "\n",
    "print('IDF-Model')\n",
    "\n",
    "model_array=[]\n",
    "scores_cv_IDF=[]\n",
    "\n",
    "#IDF Method:\n",
    "#looping on the levels and choosing the one with the highest score\n",
    "\n",
    "for x in C:\n",
    "    model  = linear_model.LogisticRegression(solver = 'sag', penalty='l2',C=x, multi_class = 'multinomial')\n",
    "    model_array.append(model.fit(train_enc_idf,train_labels))\n",
    "    \n",
    "\n",
    "for model_cv in model_array:\n",
    "    scores_cv_IDF.append(model_cv.score(dev_enc_idf,dev_labels))\n",
    "\n",
    "\n",
    "#getting the model with the highest score\n",
    "best_model_idf = model_array[np.argmax(scores_cv_IDF)]\n",
    "score_idf_model = best_model_idf.score(dev_enc_idf,dev_labels)\n",
    "\n",
    "\n",
    "print('Average Model')\n",
    "\n",
    "scores_cv_Avg=[]\n",
    "\n",
    "#Average Word Method:\n",
    "#looping on the levels and choosing the one with the highest score\n",
    "model_array=[]        \n",
    "\n",
    "for x in C:\n",
    "    model  = linear_model.LogisticRegression(solver = 'sag', penalty='l2',C=x, multi_class = 'multinomial')\n",
    "    model_array.append(model.fit(train_enc_average_word,train_labels))\n",
    "\n",
    "for model_cv in model_array:\n",
    "    scores_cv_Avg.append(model_cv.score(dev_enc_average_word,dev_labels))\n",
    "\n",
    "\n",
    "#getting the model with the highest score\n",
    "best_model_average_word = model_array[np.argmax(scores_cv_Avg)]\n",
    "score_avg_model = best_model_average_word.score(dev_enc_average_word,dev_labels)\n",
    "\n",
    "predict_y_avg = best_model_average_word.predict_proba(dev_enc_average_word)\n",
    "predict_y_idf = best_model_idf.predict_proba(dev_enc_idf)\n",
    "\n",
    "print(\"Score for Average Model in the training set is \", best_model_average_word.score(train_enc_average_word,train_labels),\\\n",
    "\" and Score for idf model in training set is\", best_model_idf.score(train_enc_idf,train_labels))\n",
    "\n",
    "print(\"Score for Average Model in the dev set is \", score_avg_model, \" and Score for idf model in the dev set is\", score_idf_model)\n",
    "print(\"Error for Average Model in the dev set is \", metrics.log_loss(dev_labels, predict_y_avg), \" and Error for idf model in the dev set is\"\\\n",
    ", metrics.log_loss(dev_labels, predict_y_idf))\n",
    "\n",
    "print(best_model_idf)#C best=0.1\n",
    "print(best_model_average_word)#Cbest=1\n",
    "\n",
    "#average model is better in the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "#we use the average model cause it's better\n",
    "output = best_model_average_word.predict(test_enc_average_word).astype(int)\n",
    "dataframe_output = pd.DataFrame()\n",
    "dataframe_output['Label'] = output\n",
    "dataframe_output['Label'].to_csv('logreg_bov_y_test_sst.txt', index=False)\n",
    "\n",
    "#####################################\n",
    "#Please note that we have already captured the lables from the trees. But we have included the \n",
    "#test predicitons from the above model in the deliverables without doing any training on the test set\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing PCA\n",
      "Performing SVC\n",
      "Building the Best Model\n",
      "SVC plus  PCA\n",
      "Score for Average Model in training set is  0.363998127340824\n",
      "Score for Average Model in dev is  0.25885558583106266\n",
      "Error for Average Model in dev is  1.6127538909155068\n",
      "Performing GB\n",
      "Building the Best Model\n",
      "Score for Average Model in TRAINING is  0.45517322097378277\n",
      "Score for Average Model in DEV is  0.329700272479564\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "#the point here is to use pca in order to deal with the sparsity of the data. having tried different methods (RF,perceptron, GB)\n",
    "#we'll later find out that GB classifier is the best one\n",
    "print('Performing PCA')\n",
    "pca = PCA(n_components=350, svd_solver='full')\n",
    "train_PCA = pca.fit_transform(train_enc_average_word)\n",
    "CV_PCA = pca.fit_transform(dev_enc_average_word)\n",
    "test_PCA = pca.fit_transform(test_enc_average_word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_array_svc=[]\n",
    "\n",
    "C =[1,2,5,0.1,0.2,0.001]\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "print('Performing SVC')\n",
    "for x in C:\n",
    "    lin_clf = svm.LinearSVC(penalty='l2', C=x)\n",
    "    clf = calibration.CalibratedClassifierCV(lin_clf) #, method = 'Simgoid', cv='prefit')\n",
    "    clf.fit(train_PCA, train_labels)\n",
    "    model_array_svc.append(clf)\n",
    "\n",
    "print('Building the Best Model')\n",
    "scores_svc=[]\n",
    "for model_cv_svc in model_array_svc:\n",
    "    scores_svc.append(model_cv_svc.score(CV_PCA,dev_labels))\n",
    "\n",
    "best_model_avw_svc = model_array_svc[np.argmax(scores_svc)]\n",
    "pred_y_proba = clf.predict_proba(CV_PCA)\n",
    "\n",
    "#predict the labels for test set\n",
    "#GB is the best one so we produce the outputs with that model, not SVC\n",
    "\n",
    "\n",
    "print(\"SVC plus  PCA\")\n",
    "print(\"Score for Average Model in training set is \", best_model_avw_svc.score(train_PCA,train_labels))\n",
    "print(\"Score for Average Model in dev is \", best_model_avw_svc.score(CV_PCA,dev_labels))\n",
    "print(\"Error for Average Model in dev is \", metrics.log_loss(dev_labels, pred_y_proba))\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "print('Performing GB')\n",
    "GB_classifier = GradientBoostingClassifier(n_estimators=40)\n",
    "GB_classifier.fit(train_enc_average_word, train_labels)\n",
    "\n",
    "print('Building the Best Model')\n",
    "pred_y_proba = GB_classifier.predict(dev_enc_average_word)\n",
    "\n",
    "print(\"Score for Average Model in TRAINING is \",GB_classifier.score(train_enc_average_word,train_labels))\n",
    "print(\"Score for Average Model in DEV is \",GB_classifier.score(dev_enc_average_word,dev_labels))\n",
    "\n",
    "\n",
    "# TYPE CODE HERE\n",
    "output = GB_classifier.predict(test_enc_average_word).astype(int)\n",
    "df_output = pd.DataFrame()\n",
    "df_output['Label'] = output\n",
    "df_output['Label'].to_csv('GB_bov_y_test_sst.txt', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rebecca\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "os.chdir('C://Users//Rebecca//Desktop//deep_learning//assignment2//nlp_project//data//SST')\n",
    "# TYPE CODE HERE\n",
    "with open('stsa.fine.train') as myfile_train:\n",
    "           train_sentences=myfile_train.read().split('\\n')\n",
    "\n",
    "with open('stsa.fine.test.X') as myfile_train:\n",
    "           test_sentences=myfile_train.read().split('\\n')\n",
    "\n",
    "with open('stsa.fine.dev') as myfile_train:\n",
    "           dev_sentences=myfile_train.read().split('\\n')\n",
    "\n",
    "def extracting_label(train_sentences):\n",
    "    train_sentence_y=list()\n",
    "    train_sentence_x=list()\n",
    "    \n",
    "    for i in range(len(train_sentences)-1):\n",
    "        train_sentence_y.append(int(train_sentences[i][0]))\n",
    "        train_sentence_x.append(train_sentences[i][1:])\n",
    "\n",
    "    return train_sentence_y,train_sentence_x\n",
    "\n",
    "train_sentence_y,train_sentence_x=extracting_label(train_sentences)\n",
    "dev_sentence_y,dev_sentence_x=extracting_label(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# We need the size of vocabulary to get maximum vocabulary \n",
    "\n",
    "train_sentence_x_hot_encoding=[]\n",
    "dev_sentence_x_hot_encoding=[]\n",
    "test_sentence_x_hot_encoding=[]\n",
    "\n",
    "for i in range(len(train_sentence_x)-1):\n",
    "    train_sentence_x_hot_encoding.append(keras.preprocessing.text.one_hot(train_sentence_x[i],5000,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',split=' '))\n",
    "for i in range(len(dev_sentence_x)-1):\n",
    "    dev_sentence_x_hot_encoding.append(keras.preprocessing.text.one_hot(dev_sentence_x[i],5000,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',split=' '))  \n",
    "for i in range(len(test_sentences)-1):\n",
    "    test_sentence_x_hot_encoding.append(keras.preprocessing.text.one_hot(test_sentences[i],5000,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',split=' '))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "44\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# To get the maximum length of a sentence we will find it out and put that as the input as maxsqlen\n",
    "\n",
    "max_train_sentence=[]\n",
    "max_dev_sentence=[]\n",
    "max_test_sentence=[]\n",
    "\n",
    "for i in range(len(train_sentence_x_hot_encoding)-1):\n",
    "    max_train_sentence.append(len(train_sentence_x_hot_encoding[i]))\n",
    "max_train=max(max_train_sentence)\n",
    "\n",
    "for i in range(len(dev_sentence_x_hot_encoding)-1):\n",
    "    max_dev_sentence.append(len(dev_sentence_x_hot_encoding[i]))\n",
    "max_dev=max(max_dev_sentence)\n",
    "\n",
    "for i in range(len(test_sentence_x_hot_encoding)-1):\n",
    "    max_test_sentence.append(len(test_sentence_x_hot_encoding[i]))\n",
    "max_test=max(max_test_sentence)\n",
    "\n",
    "\n",
    "print(max_train)\n",
    "print(max_dev)\n",
    "print(max_test)\n",
    "max_length=max_test\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(train_sentence_x_hot_encoding, maxlen=max_length, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(test_sentence_x_hot_encoding, maxlen=max_length, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
    "X_dev = keras.preprocessing.sequence.pad_sequences(dev_sentence_x_hot_encoding, maxlen=max_length, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "train_sentence_y=np_utils.to_categorical(train_sentence_y)\n",
    "dev_sentence_y=np_utils.to_categorical(dev_sentence_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rebecca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(20, dropout=0.05, recurrent_dropout=0.05)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 20  # number of hidden units in the LSTM\n",
    "vocab_size = 5000  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "    # create model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.05, dropout_U=0.05))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, None, 32)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 20)                4240      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 164,345\n",
      "Trainable params: 164,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "from keras.optimizers import SGD,Adam, RMSprop\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer_chosen       =  Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "#optimizer_chosen=RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "#optimzer_chosen=SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "\n",
    "#adam is better than sgd and rmsprop\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "\n",
    "\n",
    "\n",
    "#same as above\n",
    "    \n",
    "model.compile(loss=loss_classif,\n",
    "                optimizer=optimizer_chosen,\n",
    "                metrics=metrics_classif)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rebecca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(20, dropout=0.05, recurrent_dropout=0.05)`\n",
      "  \n",
      "C:\\Users\\Rebecca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/2\n",
      "8543/8543 [==============================] - 10s 1ms/step - loss: 1.5651 - acc: 0.2921 - val_loss: 1.5144 - val_acc: 0.3455\n",
      "Epoch 2/2\n",
      "8543/8543 [==============================] - 5s 636us/step - loss: 1.4015 - acc: 0.3804 - val_loss: 1.4390 - val_acc: 0.3627\n",
      "Accuracy: 36.27%\n",
      "0.3627272728356448\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "#bs = 64\n",
    "#n_epochs = 5\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def new_model_create():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim))\n",
    "    model.add(LSTM(nhid, dropout_W=0.05, dropout_U=0.05))\n",
    "    model.add(Dense(n_classes, activation='sigmoid'))\n",
    "    model.compile(loss=loss_classif,\n",
    "                    optimizer=optimizer_chosen,\n",
    "                    metrics=metrics_classif)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "scores_temp=0\n",
    "# ADAPT CODE BELOW\n",
    "       \n",
    "model=new_model_create()\n",
    "history = model.fit(X_train, np.array(train_sentence_y[0:8543]), batch_size=64, nb_epoch=2, validation_data=(X_dev, np.array(dev_sentence_y[0:1100])))\n",
    "scores = model.evaluate(X_dev, np.array(dev_sentence_y[0:1100]), verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "if scores[1]>scores_temp:\n",
    "    batch_size_final=64\n",
    "    epoch_final=2\n",
    "    scores_temp=scores[1]\n",
    "    \n",
    "print(scores_temp)\n",
    "\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#different values for n of epochs and batch sizes have been tried\n",
    "#We see here that model with 2 epochs and 64 batch size and adam opt, gives the besst accuracy of 36.3%\n",
    "#rmsprop gives 32.55%\n",
    "#sgd gives29.55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rebecca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(20, dropout=0.05, recurrent_dropout=0.05)`\n",
      "  \n",
      "C:\\Users\\Rebecca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/2\n",
      "8543/8543 [==============================] - 10s 1ms/step - loss: 1.5464 - acc: 0.3121 - val_loss: 1.4540 - val_acc: 0.3627\n",
      "Epoch 2/2\n",
      "8543/8543 [==============================] - 6s 745us/step - loss: 1.3442 - acc: 0.4090 - val_loss: 1.4243 - val_acc: 0.3745\n"
     ]
    }
   ],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "\n",
    "model=new_model_create()\n",
    "history = model.fit(X_train, np.array(train_sentence_y[0:8543]), batch_size=batch_size_final, nb_epoch=epoch_final, validation_data=(X_dev, np.array(dev_sentence_y[0:1100])))\n",
    "scores = model.evaluate(X_dev, np.array(dev_sentence_y[0:1100]), verbose=0)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "result=model.predict_classes(X_test)\n",
    "np.savetxt(\"logreg_lstm_y_test_sst.txt\", result, delimiter=\" \", newline = \"\\n\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 52, 32)            160000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 48, 128)           20608     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 48, 256)           263168    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 446,401\n",
      "Trainable params: 446,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rebecca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/2\n",
      "8543/8543 [==============================] - 49s 6ms/step - loss: 0.5369 - acc: 0.7938 - val_loss: 0.5024 - val_acc: 0.8000\n",
      "Epoch 2/2\n",
      "8543/8543 [==============================] - 44s 5ms/step - loss: 0.5003 - acc: 0.8000 - val_loss: 0.4962 - val_acc: 0.8000\n",
      "Training Accuracy: 0.8000\n",
      "Validation Accuracy:  0.8000\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "from keras.layers import *\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 32\n",
    "\n",
    "model_final = Sequential()\n",
    "model_final.add(layers.Embedding(5000, embedding_dim, input_length=max_test))\n",
    "model_final.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model_final.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model_final.add(layers.GlobalMaxPooling1D())\n",
    "model_final.add(layers.Dense(10, activation='relu'))\n",
    "model_final.add(layers.Dense(5, activation='sigmoid'))\n",
    "model_final.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_final.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_sz = 170\n",
    "n_epochs = 2 # different values have been tried, but after 2 epochs, the val accuracy starts to decrease\n",
    "\n",
    "\n",
    "\n",
    "history=model_final.fit(X_train, np.array(train_sentence_y[0:8543]), batch_size=batch_sz, nb_epoch=n_epochs, validation_data=(X_dev, np.array(dev_sentence_y[0:1100])))\n",
    "loss, accuracy = model_final.evaluate(X_train, np.array(train_sentence_y[0:8543]), verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_final.evaluate(X_dev, np.array(dev_sentence_y[0:1100]), verbose=False)\n",
    "print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "\n",
    "result_new=model.predict_classes(X_test)\n",
    "np.savetxt(\"Bidirectional_LSTM_y_test_sst.txt\", result_new, delimiter=\" \", newline = \"\\n\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
